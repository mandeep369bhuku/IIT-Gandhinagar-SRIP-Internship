{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Implementation of two hidden neural network classifier from scratch in JAX.\n",
    "* Two hidden layers here means (input - hidden1 - hidden2 - output).\n",
    "* You must not use flax, optax, or any other library for this task.\n",
    "* Use MNIST dataset with 80:20 train:test split.\n",
    "* Manually optimize the number of neurons in hidden layers.\n",
    "* Use gradient descent from scratch to optimize your network. You should use the Pytree\n",
    "  concept of JAX to do this elegantly.\n",
    "* Plot loss v/s iterations curve with matplotlib.\n",
    "* Evaluate the model on test data with various classification metrics and briefly discuss\n",
    "  their implications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction \n",
    "\n",
    "* What is Jax?\n",
    "  - JAX is NumPy on the CPU, GPU, and TPU, with great automatic differentiation for high-performance machine learning research.\n",
    "  - JAX can automatically differentiate native Python and NumPy code.\n",
    "  - It can differentiate through a large subset of Pythonâ€™s features, including loops, ifs, recursion, and closures, and it can     even take derivatives of derivatives of derivatives.\n",
    "  \n",
    "* What is Neural network?\n",
    "  - Neural networks reflect the behavior of the human brain, allowing computer programs to recognize patterns and solve common     problems in the fields of AI, machine learning, and deep learning.\n",
    "  - Neural networks, also knows as artificial neural networks, are a subset of machine learning and are at the heart of deep \n",
    "    deep learning algorithm.\n",
    "    \n",
    "* Details of the dataset used\n",
    "  - Dataset name: MNIST\n",
    "  - MNIST stands for Mixed National Institute of Standards and Technology, which has produced a handwritten digits dataset.         This is one of the most researched datasets in machine learning, and is used to classify handwritten digits. This dataset       is helpful for predictive analytics because of its sheer size, allowing deep learning to work its magic efficiently.\n",
    "  - Information: * name : MNIST * length : 70000\n",
    "    Input Summary: * shape : (28, 28, 1) * range : (0.0, 1.0)\n",
    "    Target Summary: * shape : (10,) * range : (0.0, 1.0)\n",
    "    \n",
    "* Pipeline \n",
    "  - We will first specify and train a simple MLP on MNIST using JAX for the computation. We will use PyTorch's data loading API     to load images and labels.\n",
    "\n",
    "  - Of course, we can use JAX with any API that is compatible with NumPy to make specifying the model a bit more plug-and-         play. Here, just for explanatory purposes, we won't use any neural network libraries or special APIs for building our           model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lM2ZsxxqPo5M"
   },
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, vmap\n",
    "from jax import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters\n",
    "* Let's get a few bookkeeping items out of the way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oey2sPhGP0eZ"
   },
   "outputs": [],
   "source": [
    "# A helper function to randomly initialize weights and biases\n",
    "# for a dense neural network layer\n",
    "def random_layer_params(m, n, key, scale=1e-2):\n",
    "  w_key, b_key = random.split(key)\n",
    "  return scale * random.normal(w_key, (n, m)), scale * random.normal(b_key, (n,))\n",
    "\n",
    "# Initialize all layers for a fully-connected neural network with sizes \"sizes\"\n",
    "def init_network_params(sizes, key):\n",
    "  keys = random.split(key, len(sizes))\n",
    "  return [random_layer_params(m, n, k) for m, n, k in zip(sizes[:-1], sizes[1:], keys)]\n",
    "\n",
    "layer_sizes = [784, 512, 512, 10]\n",
    "step_size = 0.01\n",
    "num_epochs = 10\n",
    "batch_size = 128\n",
    "n_targets = 10\n",
    "params = init_network_params(layer_sizes, random.PRNGKey(0))\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dense(512, activation=tf.nn.tanh),\n",
    "  tf.keras.layers.Dense(512, activation=tf.nn.tanh),\n",
    "  tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto-batching predictions\n",
    "\n",
    "Let us first define our prediction function. Note that we're defining this for a _single_ image example. We're going to use JAX's `vmap` function to automatically handle mini-batches, with no performance penalty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LCNd3oH9P9i2"
   },
   "outputs": [],
   "source": [
    "\n",
    "from jax.scipy.special import logsumexp\n",
    "\n",
    "def relu(x):\n",
    "  return jnp.maximum(0, x)\n",
    "\n",
    "def predict(params, image):\n",
    "  # per-example predictions\n",
    "  activations = image\n",
    "  for w, b in params[:-1]:\n",
    "    outputs = jnp.dot(w, activations) + b\n",
    "    activations = relu(outputs)\n",
    "  \n",
    "  final_w, final_b = params[-1]\n",
    "  logits = jnp.dot(final_w, activations) + final_b\n",
    "  return logits - logsumexp(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that our prediction function only works on single images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D3125BkMQBAm",
    "outputId": "928b2786-3583-4f3e-bcc4-a1428bfb1475"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "# This works on single examples\n",
    "random_flattened_image = random.normal(random.PRNGKey(1), (28 * 28,))\n",
    "preds = predict(params, random_flattened_image)\n",
    "print(preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CWT0P-GJQCCg",
    "outputId": "b25dc09f-8b9a-4da0-a7ca-4dd48aa04b3a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid shapes!\n"
     ]
    }
   ],
   "source": [
    "# Doesn't work with a batch\n",
    "random_flattened_images = random.normal(random.PRNGKey(1), (10, 28 * 28))\n",
    "try:\n",
    "  preds = predict(params, random_flattened_images)\n",
    "except TypeError:\n",
    "  print('Invalid shapes!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C6TwUspKQF84",
    "outputId": "be51c6c7-a99a-43ef-b3b0-7818d9c0b6d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 10)\n"
     ]
    }
   ],
   "source": [
    "# Let's upgrade it to handle batches using `vmap`\n",
    "\n",
    "# Make a batched version of the `predict` function\n",
    "batched_predict = vmap(predict, in_axes=(None, 0))\n",
    "\n",
    "# `batched_predict` has the same call signature as `predict`\n",
    "batched_preds = batched_predict(params, random_flattened_images)\n",
    "print(batched_preds.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we have all the ingredients we need to define our neural network and train it. We've built an auto-batched version of `predict`, which we should be able to use in a loss function. We should be able to use `grad` to take the derivative of the loss with respect to the neural network parameters. Last, we should be able to use `jit` to speed up everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QGTwuCyZQH8h"
   },
   "outputs": [],
   "source": [
    "def one_hot(x, k, dtype=jnp.float32):\n",
    "  \"\"\"Create a one-hot encoding of x of size k.\"\"\"\n",
    "  return jnp.array(x[:, None] == jnp.arange(k), dtype)\n",
    "  \n",
    "def accuracy(params, images, targets):\n",
    "  target_class = jnp.argmax(targets, axis=1)\n",
    "  predicted_class = jnp.argmax(batched_predict(params, images), axis=1)\n",
    "  return jnp.mean(predicted_class == target_class)\n",
    "\n",
    "def loss(params, images, targets):\n",
    "  preds = batched_predict(params, images)\n",
    "  return -jnp.mean(preds * targets)\n",
    "\n",
    "@jit\n",
    "def update(params, x, y):\n",
    "  grads = grad(loss)(params, x, y)\n",
    "  return [(w - step_size * dw, b - step_size * db)\n",
    "          for (w, b), (dw, db) in zip(params, grads)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading with `tensorflow/datasets`\n",
    "\n",
    "JAX is laser-focused on program transformations and accelerator-backed NumPy, so we don't include data loading or munging in the JAX library. There are already a lot of great data loaders out there, so let's just use them instead of reinventing anything. We'll use the `tensorflow/datasets` data loader.\n",
    "\n",
    "sklearn.model.selection may also be used to import MNIST Dataset. Any type of library can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BTkJOHe3QJ5R"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.datasets import mnist\n",
    "\n",
    "# Ensure TF does not see GPU and grab all GPU memory.\n",
    "tf.config.set_visible_devices([], device_type='GPU')\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "data_dir = '/tmp/tfds'\n",
    "\n",
    "# Fetch full datasets for evaluation\n",
    "# tfds.load returns tf.Tensors (or tf.data.Datasets if batch_size != -1)\n",
    "# You can convert them to NumPy arrays (or iterables of NumPy arrays) with tfds.dataset_as_numpy\n",
    "mnist_data, info = tfds.load(name=\"mnist\", batch_size=-1, data_dir=data_dir, with_info=True)\n",
    "mnist_data = tfds.as_numpy(mnist_data)\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x = jnp.concatenate((x_train, x_test))\n",
    "y = jnp.concatenate((y_train, y_test))\n",
    "train_size = 0.8\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=train_size, random_state=42)\n",
    "num_labels = info.features['label'].num_classes\n",
    "h, w, c = info.features['image'].shape\n",
    "num_pixels = h * w * c\n",
    "\n",
    "# Full train set\n",
    "train_images, train_labels = x_train, y_train\n",
    "train_images = jnp.reshape(train_images, (len(train_images), num_pixels))\n",
    "train_labels = one_hot(train_labels, num_labels)\n",
    "\n",
    "# Full test set\n",
    "test_images, test_labels = x_test, y_test\n",
    "test_images = jnp.reshape(test_images, (len(test_images), num_pixels))\n",
    "test_labels = one_hot(test_labels, num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1sweFa_wQL9m",
    "outputId": "2543c004-c686-4871-bc08-39ed76245017"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (56000, 784) (56000, 10)\n",
      "Test: (14000, 784) (14000, 10)\n"
     ]
    }
   ],
   "source": [
    "print('Train:', train_images.shape, train_labels.shape)\n",
    "print('Test:', test_images.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g-1x6DB6QQUJ",
    "outputId": "83e71a15-4153-41b9-8934-f31005e4da6b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 in 10.03 sec\n",
      "Training set accuracy 0.9253214001655579\n",
      "Test set accuracy 0.9264999628067017\n",
      "Epoch 1 in 8.86 sec\n",
      "Training set accuracy 0.942339301109314\n",
      "Test set accuracy 0.9432142972946167\n",
      "Epoch 2 in 9.21 sec\n",
      "Training set accuracy 0.9525356888771057\n",
      "Test set accuracy 0.9540714025497437\n",
      "Epoch 3 in 9.27 sec\n",
      "Training set accuracy 0.9593035578727722\n",
      "Test set accuracy 0.9597142934799194\n",
      "Epoch 4 in 9.77 sec\n",
      "Training set accuracy 0.9646071195602417\n",
      "Test set accuracy 0.9639999866485596\n",
      "Epoch 5 in 10.34 sec\n",
      "Training set accuracy 0.9685892462730408\n",
      "Test set accuracy 0.9672142863273621\n",
      "Epoch 6 in 9.42 sec\n",
      "Training set accuracy 0.9718571305274963\n",
      "Test set accuracy 0.9700714349746704\n",
      "Epoch 7 in 9.14 sec\n",
      "Training set accuracy 0.9746785759925842\n",
      "Test set accuracy 0.9722856879234314\n",
      "Epoch 8 in 9.36 sec\n",
      "Training set accuracy 0.9769642949104309\n",
      "Test set accuracy 0.9750714302062988\n",
      "Epoch 9 in 9.27 sec\n",
      "Training set accuracy 0.9791249632835388\n",
      "Test set accuracy 0.9772142767906189\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def get_train_batches():\n",
    "  # as_supervised=True gives us the (image, label) as a tuple instead of a dict\n",
    "  ds = tfds.load(name='mnist', split='train', as_supervised=True, data_dir=data_dir)\n",
    "  # You can build up an arbitrary tf.data input pipeline\n",
    "  ds = ds.batch(batch_size).prefetch(1)\n",
    "  # tfds.dataset_as_numpy converts the tf.data.Dataset into an iterable of NumPy arrays\n",
    "  return tfds.as_numpy(ds)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "  start_time = time.time()\n",
    "  for x, y in get_train_batches():\n",
    "    x = jnp.reshape(x, (len(x), num_pixels))\n",
    "    y = one_hot(y, num_labels)\n",
    "    params = update(params, x, y)\n",
    "  epoch_time = time.time() - start_time\n",
    "\n",
    "  train_acc = accuracy(params, train_images, train_labels)\n",
    "  test_acc = accuracy(params, test_images, test_labels)\n",
    "  print(\"Epoch {} in {:0.2f} sec\".format(epoch, epoch_time))\n",
    "  print(\"Training set accuracy {}\".format(train_acc))\n",
    "  print(\"Test set accuracy {}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've now used the whole of the JAX API: grad for derivatives, jit for speedups and vmap for auto-vectorization. We used NumPy to specify all of our computation, and borrowed the great data loaders from tensorflow/datasets, and ran the whole thing on the GPU."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "neural networks.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
