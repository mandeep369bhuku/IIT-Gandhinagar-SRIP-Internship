# IIT-Gandhinagar-SRIP-Internship
### About
My qualifications and skills, I believe, make me an excellent candidate for the job. I am a full-time student at Madhuben and Bhanubhai Patel Institute of Technology in Anand, Gujarat. I'm in my third year of college. In my situation, the CPI is 8.7.

I'm now working on a conference paper on Twitter sentiment analysis with Mr. Deepak Gerard, a Doctoral Research Scholar at the National Institute of Technology, Tiruchirappalli (NIT Trichy). I'm also pursuing an Indian Institute of Technology, Madras, online B.Sc. in data science and programming. In July, I was fortunate enough to earn an internship at Larsen & Toubro. I learnt the fundamentals of machine learning and worked on projects including the Flair prediction project, House price prediction with datasets, and Titanic survival prediction project during my one-month internship. Prior to it, I worked on the Hawkeye project. It's a professional aim training programme for gamers and professional drivers to assist them enhance their reflexes and reaction speeds. It was made with Java, JavaFxml, and Scenebulider. At the A.D.I.T. International Mathematical Conference, I also presented a research work. I competed in a nationwide coding competition offered by G.C.E.T.

As a consequence of my intrinsic interest in the subjects and further practise over the past 2.5 years, I have reached an intermediate level of competency in C, C++, Java, and Python. I enrolled in a 'Natural Language Processing' course to learn more about the subject. Throughout my education, I learnt a variety of important life skills, one of which was the value of persevering hard work. And by working hard on the tasks supplied, I was able to learn new things in JAX and BlackJAX, which I tried out in Tasks 3 and 4 of the tasks.

### **Task 3**

I used a neural network model with two input labels for this task. We were told not to use certain libraries such as flax, optax, and others. Because these libraries were unfamiliar to me, I was just interested in getting a general overview of them. Then I loaded the MNIST dataset and divided it into 80:20 training and testing ratios. In addition, I manually optimised the number of neurons in hidden layers to improve model accuracy. As Pytree was new to me, I couldnâ€™t complete that task in given time. I didn't make a loss vs iteration curve because I didn't create gradient descent from scratch. To analyse the model, I have used various evaluation criteria such as accuracy and loss.



### **Task 4**

In task 4, I initially created a 1D feature matrix and 1D data labels, as well as some noise, to create a complete dataset. After that, I sampled datapoints from the produced dataset using BlackJAX's RMH sampling method. The primary problem was sampling with BlackJAX, which I did my best to do, and I also realised that I needed to plot the prediction mean alongside the datapoints and SDs.
